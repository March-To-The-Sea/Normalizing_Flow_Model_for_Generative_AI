{"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"","display_name":""},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"def Coupling():\n    input_layer = layers.Input(shape = 2)\n\n    s_layer_1 = layers.Dense(256, activation = \"relu\", kernel_regularizer = regularizers.l2(0.01))(input_layer)\n    s_layer_2 = layers.Dense(256, activation = \"relu\", kernel_regularizer = regularizers.l2(0.01))(s_layer_1)\n    s_layer_3 = layers.Dense(256, activation = \"relu\", kernel_regularizer = regularizers.l2(0.01))(s_layer_2)\n    s_layer_4 = layers.Dense(256, activation = \"relu\", kernel_regularizer = regularizers.l2(0.01))(s_layer_3)\n    s_layer_5 = layers.Dense(2, activation = \"tanh\", kernel_regularizer = regularizers.l2(0.01))(s_layer_4)\n\n    s_layer_1 = layers.Dense(256, activation = \"relu\", kernel_regularizer = regularizers.l2(0.01))(input_layer)\n    t_layer_2 = layers.Dense(256, activation = \"relu\", kernel_regularizer = regularizers.l2(0.01))(t_layer_1)\n    t_layer_3 = layers.Dense(256, activation = \"relu\", kernel_regularizer = regularizers.l2(0.01))(t_layer_2)\n    t_layer_4 = layers.Dense(256, activation = \"relu\", kernel_regularizer = regularizers.l2(0.01))(t_layer_3)\n    t_layer_5 = layers.Dense(2, activation = \"tanh\", kernel_regularizer = regularizers.l2(0.01))(t_layer_4)\n\n    return models.Model(inputs = input_layer, outputs = [s_layer_5, t_layer_5])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RealNVP(models.Model):\n    def __init__(self, input_dim, coupling_layers, coupling_dim, regularization):\n        super(RealNVP, self).__init__()\n        self.coupling_layers = coupling_layers\n        self.distribution = tfp.distributions.MultivariateNormalDiag(\n            loc = [0.0, 0.0], scale_diag = [1.0, 1.0]\n        )\n        self.masks = np.array(\n            [[0, 1], [1, 0]] * (coupling_layers // 2), dtype = \"float32\"\n        )\n        self.loss_tracker = metrics.Mean(name = \"loss\")\n        self.layers-list = [\n            Coupling(input_dim, coupling_dim, regularization)\n            for i in range(coupling_layers)\n        ]\n\n        @property\n        def metrics(self):\n            return [self.loss_tracker]\n        def call(self, x, training = True):\n            log_det_inv = 0\n            direction = 1\n            if training:\n                direction = -1\n            for i in range(self.coupling_layers)[::direction]:\n                x_masked = x * self.masks[i]\n                reversed_mask = 1 - self.masks[i]\n                s, t = self.layers_list[i](x_masked)\n                s *= reversed_mask\n                t *= reversed_mask\n                gate = (direction - 1) / 2\n                x = (\n                    reversed_mask\n                    * (x * tf.exp(direction * s) + direction * t * tf.exp(gate * s))\n                    + x_masked\n                )\n                log_det_inv += gate * tf.reduce_sum(s, axis = 1)\n            return x, log_det_inv\n            \n        def log_loss(self, x):\n            y, logdet = self(x)\n            log_likelihood = self.distribution.log_prob(y) + logdet\n            return -tf.reduce_mean(log_likelihood)\n        \n        def train_step(self, data):\n            with tf.GradientTape() as tape:\n                loss = self.log_loss(data)\n            g = tape.gradient(loss, self.trainable_variables)\n            self.optimizer.apply_gradients(zip(g, self.trainable_variables))\n            self.loss_tracker.update_state(loss)\n            return {\"loss\": self.loss_tracker.result()}\n        \nmodel = RealNVP(\n    input_dim = 2,\n    coupling_layers = 6,\n    coupling_dim = 256,\n    retularization = 0.01\n)\n\nmodel.compile(optimizer = optimizers.Adam(learning_rate = 0.0001))\n\nmodel.fit(\n    normalized_data,\n    batch_size = 256,\n    epochs = 300\n)","metadata":{},"execution_count":null,"outputs":[]}]}